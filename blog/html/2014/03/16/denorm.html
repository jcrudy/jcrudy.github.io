<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="This blog is pretty okay">
        <meta name="viewport" content="width=device-width">
        <title>Transformations and consequences &mdash; A fistful of data</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/modern5.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/webfont.css" type="text/css">
        <link rel="shortcut icon" href="../../../_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="next" title="Boosting with MARS" href="../../02/07/boosting_with_mars.html" /><link rel="prev" title="How to sample from a hazard function" href="../../04/11/sampling_from_an_arbitrary_hazard_function.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.2.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Adjusts document height if sidebar is taller
            var documentwrapper = document.getElementsByTagName("article")[0];
            var sidebar = document.getElementsByClassName("sidebar")[0];

            if (sidebar.offsetHeight > documentwrapper.offsetHeight)
            {
                var mainwrapper = document.getElementsByClassName("main")[0];
                documentwrapper.style.minHeight = mainwrapper.offsetHeight + "px";
            }

            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header>
            <hgroup>
              <h1><a href="../../../index.html">A fistful of data</a></h1><h2>Data wrangling, rodeo metaphors, and tng references</h2></hgroup>
          </header>
      <nav>
            <ul><li class="quicklink"><div class="rss">
        <a href="../../../rss.html" title="Subscribe via RSS"><span class="webfont">B</span></a>
    </div></li><li class="main-nav">
                  <a href="../../../index.html">Home</a>
                </li>
              <li class="main-nav">
                  <a href="../../../pages/my_software.html">Software</a>
                </li>
              <li class="main-nav">
                  <a href="../../../pages/about.html">About me</a>
                </li>
              </ul>
          </nav><div class="main-container"><div class="main wrapper clearfix"><article><ul class="related clearfix">
            <li class="left"> &laquo; <a href="../../04/11/sampling_from_an_arbitrary_hazard_function.html">How to sample from a hazard function</a></li>
            <li class="right"><a href="../../02/07/boosting_with_mars.html">Boosting with MARS</a> &raquo; </li>
        </ul>
    <div class="timestamp postmeta">
            <span>March 16, 2014</span>
        </div>
    <div class="section" id="transformations-and-consequences">
<h1>Transformations and consequences</h1>
<blockquote class="epigraph">
<div><p>He knows she tried to be forgiving, but who can just shrug away a guilty lie,
a stab in the back? Such a mistake will change a relationship irreversibly,
even if we have learned from the mistake and would never repeat it.</p>
<p class="attribution">&mdash;In game text from Braid</p>
</div></blockquote>
<p>Today I want to talk about mistakes.  No, not mistakes, really.  A mistake is something we regret, something with lingering consequences, like when you work too much on a laptop and get a painful neck spasm that lasts the rest of the week.  A mistake is an action that, if we had the chance to go back and try again, we would do differently.  But there are some actions that we regret, whose consequences linger painfully, but that we wouldn&#8217;t do differently even if we somehow had the chance.  In either case we limp along as best we can, trying to compensate for that which is irretrievably lost.</p>
<p>I&#8217;m alluding, of course, to the Box-Cox transformation, and, more generally, to all those nonlinear monotonic transformations which may be applied to a response variable in a regression problem to make the regression perform better.  These kinds of transformations can sometimes mark the difference between a model that is quite predictive and one that is over fit and not at all predictive on the testing set.  The problem, of course, is that once you build your model on the transformed response variable, it becomes difficult to make predications on the original response scale.  One can&#8217;t simply apply the inverse of the original transformation to the predictions and expect everything to be fine.  Everything will not be fine.  Today I want to talk about what happens after the nonlinear monotonic transformation.  It wasn&#8217;t a mistake.  It&#8217;s just a mess, but we&#8217;re in this mess together.</p>
<p>Let&#8217;s say we&#8217;ve got some response <span class="math">\(y\)</span> and a nonlinear monotonic transformation <span class="math">\(f\left(\right)\)</span>.  Let&#8217;s have <span class="math">\(z=f\left(y\right)\)</span> and say we have some pretty good predictor <span class="math">\(\hat{z}\)</span> of <span class="math">\(E\left(z\right)\)</span>.  Firstly, what makes this predictor pretty good?  How can we compare it to some other predictor, <span class="math">\(\hat{y}\)</span>, that we built for <span class="math">\(E\left(y\right)\)</span> on the original scale?  It&#8217;s a popular practice to use the coefficient of determination, <span class="math">\(R^2\)</span>, to assess the predictive power of a regression model, but <span class="math">\(R^2\)</span> is very sensitive to nonlinear transformations.  A better idea in this context is to use a rank based measure of predictive power, such as Spearman&#8217;s <span class="math">\(\rho\)</span> or Kendall&#8217;s <span class="math">\(\tau\)</span>.  These metrics will not be influenced by any monotonic transformation.  I also like to compare <span class="math">\(R^2\)</span> between the training set and a reserved testing set for my regression models.  Often I will find extreme overfitting on the original response scale while the model I fit on the transformed response shows good generalization.</p>
<p>So we&#8217;ve assessed predictiveness and generalization for our models, and the best one is <span class="math">\(\hat{z}\)</span>.  Great.  That means we have a model for <span class="math">\(E\left(f\left(y\right)\right)\)</span> and we want a model for <span class="math">\(E\left(y\right)\)</span>.  We know <span class="math">\(f\left(y\right)\)</span>, and since it&#8217;s monotonic we can presumably find the inverse <span class="math">\(f^{-1}\left(z\right)=y\)</span>.  However, <span class="math">\(E\left(f\left(y\right)\right)\ne f\left(E\left(y\right)\right)\)</span> in general, so it is also not generally true that <span class="math">\(f^{-1}\left(E\left(f\left(y\right)\right)\right) = E\left(y\right)\)</span>.  In fact it is usually false.  Super false.  It turns out that this data transformation problem is similar to another problem about which a group of scientists in La Jolla has recently published a few short articles: risk model calibration.  I stumbled upon their publications recently when I was researching the topic of this post <a class="reference internal" href="#jiang2011">[1]</a><a class="reference internal" href="#jiang2011a">[2]</a><a class="reference internal" href="#wu2012">[3]</a>.  The problems are essentially identical: given a predictive model with good rank-based performance (area under the ROC curve in the case of classifiers, Spearman&#8217;s <span class="math">\(\rho\)</span> or Kendall&#8217;s <span class="math">\(tau\)</span> in the case of regression problems with numeric response), find a transformation such that the output of that model accurately predicts the expected value of the quantity of interest.  Generally, a monotonic transformation is desired so that the rank-based performance is preserved, but one of the three methods described by the lab in question actually gives a transformation that is not strictly monotonic.  I don&#8217;t want to describe the methods in detail here.  The papers I cited are freely available online.  I mostly just want to share a few methods I implemented that are very similar (and were inspired by these scientists&#8217; fine work).</p>
<div class="section" id="an-example">
<h2>An example</h2>
<p>First I&#8217;m going to simulate a simple regression problem with a lognormal response distribution.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">calibrators</span> <span class="kn">import</span> <span class="n">SmoothIso</span><span class="p">,</span> <span class="n">SmoothMovingAverage</span><span class="p">,</span> <span class="n">spearman</span><span class="p">,</span> <span class="n">kendall</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># Generate some fake data with a lognormal distribution</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
<span class="n">eta</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">eta</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, I&#8217;m going to fit a linear regression model to the simulated data on the log scale.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Do a linear regression on the log of the data</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">z</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">z_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">)</span>
</pre></div>
</div>
<p>The goal now is to find a way to go from <span class="code docutils literal"><span class="pre">z_hat</span></span> back to some estimate of <span class="code docutils literal"><span class="pre">y</span></span>.  Of course we could have just done linear regression on the data scale directly, like this.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Try doing linear regression directly</span>
<span class="n">beta_hat_direct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_hat_direct</span><span class="p">)</span>

<span class="c"># Compare the two models</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">spearman</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z_hat</span><span class="p">)</span>
<span class="n">rho_direct</span> <span class="o">=</span> <span class="n">spearman</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">kendall</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z_hat</span><span class="p">)</span>
<span class="n">tau_direct</span> <span class="o">=</span> <span class="n">kendall</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;rho is </span><span class="si">%f</span><span class="s"> for the log model and </span><span class="si">%f</span><span class="s"> for the direct model&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">rho_direct</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;tau is </span><span class="si">%f</span><span class="s"> for the log model and </span><span class="si">%f</span><span class="s"> for the direct model&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">tau_direct</span><span class="p">)</span>
<span class="c"># rho is 0.802388 for the log model and 0.800836 for the direct model</span>
<span class="c"># tau is 0.606838 for the log model and 0.605232 for the direct model</span>
</pre></div>
</div>
<p>In this case, the difference in rank-based performance between the two models is not significant.  I just wanted to show how the comparison might be made.  When using more complex nonparametric methods, data scale has a significant effect on generalization ability of the fitted models.  With linear regression on this particular problem, overfitting is not really an issue.  Now I want to try out some different ways of reversing the log transformation on the predictions.  It turns out that for this particular problem, where the data have a known lognormal distribution, the exact right answer is known.  That is, it is a provable fact that <span class="math">\(E\left(y\right) = e^{E\left(z\right) + \frac{\sigma^2}{2}}\)</span>, where <span class="math">\(\sigma\)</span> is the known scale parameter of the lognormal distribution.  I will use this result as a basis for comparison.  First, I&#8217;m going to try using the obvious and wrong inverse, <span class="math">\(\hat{y} = e^{\hat{z}}\)</span>.  Next, I&#8217;m going to use the two methods I implemented, SmoothIso (based on the idea from <a class="reference internal" href="#jiang2011a">[2]</a>) and SmoothMovingAverage (based more loosely on the idea from <a class="reference internal" href="#jiang2011">[1]</a>).  Finally, I&#8217;ll plot the results together to see how they compare.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Range for plotting calibration curves</span>
<span class="n">z_range</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">z_hat</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="o">.</span><span class="mo">05</span><span class="p">)</span>

<span class="c"># Try reversing the log by inversion</span>
<span class="n">y_hat_inv</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Try reversing the log by the actual correct formula</span>
<span class="n">y_hat_correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_range</span> <span class="o">+</span> <span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c"># Try reversing the log using SmoothIso</span>
<span class="n">smooth_iso</span> <span class="o">=</span> <span class="n">SmoothIso</span><span class="p">(</span><span class="n">max_degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat_si</span> <span class="o">=</span> <span class="n">smooth_iso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Try reversing the log using SmoothMovingAverage</span>
<span class="n">moving_average</span> <span class="o">=</span> <span class="n">SmoothMovingAverage</span><span class="p">(</span><span class="n">max_degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat_sma</span> <span class="o">=</span> <span class="n">moving_average</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Plot the different reversal attempts</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">&#39;k.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;data&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="s">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;direct regression&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_inv</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;$e^{\hat{z}}$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_correct</span><span class="p">,</span> <span class="s">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;$e^{\hat{z} + \sigma^{2}/2 }$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_si</span><span class="p">,</span> <span class="s">&#39;y&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;smooth iso&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_sma</span><span class="p">,</span> <span class="s">&#39;g--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;smooth moving average&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">&#39;example.png&#39;</span><span class="p">,</span> <span class="n">transparent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the resulting plot.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../../_images/example.png"><img alt="`Plot of the different calibration methods`" src="../../../_images/example.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
</div>
<div class="section" id="final-thoughts">
<h2>Final thoughts</h2>
<p>The plot illustrates nicely that simply inverting the original transformation (solid red line) is a very bad idea.  On this data set both the calibration methods stick near the correct (dashed red) curve, and I have found them to be effective for real data sets as well.  What&#8217;s nice about both calibration methods is that they are purely data driven.  They will work just as well in situations in which the exact distribution of the data is either not convenient or not known, or even in situations in which the original transformation is unknown.  Instead of making distributional assumptions and trying to solve integrals, you can just plug in these calibration methods and most likely end up with a better result.</p>
<p>So, how does all this work?  The exact details are beyond the scope of this particular blog entry, but generally speaking SmoothIso works by performing isotonic regression followed by MARS, and SmoothMovingAverage works by training a MARS model on a moving average of the training data.  I encourage you to read the articles and check out my implementations.  I&#8217;m putting all the code used in this post, including the calibrators themselves, in a <a class="reference external" href="https://github.com/jcrudy/calibrators">github repository</a>.  I hope you&#8217;ll try it.</p>
<p id="bibtex-bibliography-2014/03/16/denorm-0"><table class="docutils citation" frame="void" id="jiang2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine.. <em>Journal of the American Medical Informatics Association</em>, 19(2):263–274, 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jiang2011a" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id4">2</a>)</em> Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Smooth isotonic regression: a new method to calibrate predictive models.. <em>AMIA Summits Transl Sci Proc. 2011</em>, 2011:16–20, January 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wu2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Yuan Wu, Xiaoqian Jiang, Jihoon Kim, and Lucila Ohno-machado. I-spline Smoothing for Calibrating Predictive Models. <em>AMIA Summits Transl Sci Proc. 2012</em>, pages 39–46, 2012.</td></tr>
</tbody>
</table>
</p>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Jason Rudy</span>
        </div>
        <div class="categories">
            <span>
                Filed under:
                <a href="../../../categories/python.html">python</a>, <a href="../../../categories/statistics.html">statistics</a></span>
        </div>
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/transformation.html">transformation</a>, <a href="../../../tags/calibration.html">calibration</a></span>
        </div>
        </div>
    <div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "jcrudy-blog";    var disqus_identifier = "2014/03/16/denorm";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <script data-gittip-username="jcrudy"
        data-gittip-widget="button"
        src="//gttp.co/v1.js"></script>
<ul class="related clearfix">
            <li class="left"> &laquo; <a href="../../04/11/sampling_from_an_arbitrary_hazard_function.html">How to sample from a hazard function</a></li>
            <li class="right"><a href="../../02/07/boosting_with_mars.html">Boosting with MARS</a> &raquo; </li>
        </ul></article><aside class="sidebar"><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="../../04/11/sampling_from_an_arbitrary_hazard_function.html">How to sample from a hazard function</a>
        </li><li>
            <a href="#">Transformations and consequences</a>
        </li><li>
            <a href="../../02/07/boosting_with_mars.html">Boosting with MARS</a>
        </li><li>
            <a href="../../../2013/12/08/introduction_to_iscala.html">Introduction to IScala</a>
        </li><li>
            <a href="../../../2013/12/02/blogstart.html">Blogstart</a>
        </li></ul>
</div>
</section><section><div class="widget" id="searchbox">
    <h1>Search</h1>
    <form action="../../../search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="webfont">L</span></button>
    </form>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container"><footer class="wrapper">&copy; Copyright 2013 and 2014, Jason Rudy. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>