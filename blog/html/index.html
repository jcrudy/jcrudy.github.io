<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="This blog is pretty okay">
        <meta name="viewport" content="width=device-width">
        <title>Home &mdash; A fistful of data</title>
            <link rel="stylesheet" href="_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="_static/main.css" type="text/css">
            <link rel="stylesheet" href="_static/modern5.css" type="text/css">
            <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="_static/webfont.css" type="text/css">
        <link rel="shortcut icon" href="_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="_static/plugins.js"></script>
        <script src="_static/main.js"></script>
        <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.2.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="_static/underscore.js"></script><script type="text/javascript" src="_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="_static/disqus.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Adjusts document height if sidebar is taller
            var documentwrapper = document.getElementsByTagName("article")[0];
            var sidebar = document.getElementsByClassName("sidebar")[0];

            if (sidebar.offsetHeight > documentwrapper.offsetHeight)
            {
                var mainwrapper = document.getElementsByClassName("main")[0];
                documentwrapper.style.minHeight = mainwrapper.offsetHeight + "px";
            }

            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header>
            <hgroup>
              <h1><a href="#">A fistful of data</a></h1><h2>Data wrangling, rodeo metaphors, and tng references</h2></hgroup>
          </header>
      <nav>
            <ul><li class="quicklink"><div class="rss">
        <a href="rss.html" title="Subscribe via RSS"><span class="webfont">B</span></a>
    </div></li><li class="main-nav">
                  <a href="#">Home</a>
                </li>
              <li class="main-nav">
                  <a href="pages/my_software.html">Software</a>
                </li>
              <li class="main-nav">
                  <a href="pages/about.html">About me</a>
                </li>
              </ul>
          </nav><div class="main-container"><div class="main wrapper clearfix"><article><div class="timestamp postmeta">
            <span>March 16, 2014</span>
        </div>
        <div class="section" id="transformations-and-consequences">
<h1><a href="2014/03/16/denorm.html">Transformations and consequences</a></h1>
<blockquote class="epigraph">
<div><p>He knows she tried to be forgiving, but who can just shrug away a guilty lie,
a stab in the back? Such a mistake will change a relationship irreversibly,
even if we have learned from the mistake and would never repeat it.</p>
<p class="attribution">—In game text from Braid</p>
</div></blockquote>
<p>Today I want to talk about mistakes.  No, not mistakes, really.  A mistake is something we regret, something with lingering consequences, like when you work too much on a laptop and get a painful neck spasm that lasts the rest of the week.  A mistake is an action that, if we had the chance to go back and try again, we would do differently.  But there are some actions that we regret, whose consequences linger painfully, but that we wouldn’t do differently even if we somehow had the chance.  In either case we limp along as best we can, trying to compensate for that which is irretrievably lost.</p>
<p>I’m alluding, of course, to the Box-Cox transformation, and, more generally, to all those nonlinear monotonic transformations which may be applied to a response variable in a regression problem to make the regression perform better.  These kinds of transformations can sometimes mark the difference between a model that is quite predictive and one that is over fit and not at all predictive on the testing set.  The problem, of course, is that once you build your model on the transformed response variable, it becomes difficult to make predications on the original response scale.  One can’t simply apply the inverse of the original transformation to the predictions and expect everything to be fine.  Everything will not be fine.  Today I want to talk about what happens after the nonlinear monotonic transformation.  It wasn’t a mistake.  It’s just a mess, but we’re in this mess together.</p>
<p>Let’s say we’ve got some response <span class="math">\(y\)</span> and a nonlinear monotonic transformation <span class="math">\(f\left(\right)\)</span>.  Let’s have <span class="math">\(z=f\left(y\right)\)</span> and say we have some pretty good predictor <span class="math">\(\hat{z}\)</span> of <span class="math">\(E\left(z\right)\)</span>.  Firstly, what makes this predictor pretty good?  How can we compare it to some other predictor, <span class="math">\(\hat{y}\)</span>, that we built for <span class="math">\(E\left(y\right)\)</span> on the original scale?  It’s a popular practice to use the coefficient of determination, <span class="math">\(R^2\)</span>, to assess the predictive power of a regression model, but <span class="math">\(R^2\)</span> is very sensitive to nonlinear transformations.  A better idea in this context is to use a rank based measure of predictive power, such as Spearman’s <span class="math">\(\rho\)</span> or Kendall’s <span class="math">\(\tau\)</span>.  These metrics will not be influenced by any monotonic transformation.  I also like to compare <span class="math">\(R^2\)</span> between the training set and a reserved testing set for my regression models.  Often I will find extreme overfitting on the original response scale while the model I fit on the transformed response shows good generalization.</p>
<p>So we’ve assessed predictiveness and generalization for our models, and the best one is <span class="math">\(\hat{z}\)</span>.  Great.  Now we have a real mess to clean up.  We have a model for <span class="math">\(E\left(f\left(y\right)\right)\)</span> and we want a model for <span class="math">\(E\left(y\right)\)</span>.  We’ve know <span class="math">\(f\left(y\right)\)</span>, and since it’s monotonic we can presumably find the inverse <span class="math">\(f^{-1}\left(z\right)=y\)</span>.  However, <span class="math">\(E\left(f\left(y\right)\right)\ne f\left(E\left(y\right)\right)\)</span> in general, so it is also not generally true that <span class="math">\(f^{-1}\left(E\left(f\left(y\right)\right)\right) = E\left(y\right)\)</span>.  In fact it is usually false.  Super false.  It turns out that this data transformation problem is similar to another problem about which a group of scientists in La Jolla has recently published a few short articles: risk model calibration.  I stumbled upon their publications recently when I was researching the topic of this post <a class="reference internal" href="2014/03/16/denorm.html#jiang2011">[1]</a><a class="reference internal" href="2014/03/16/denorm.html#jiang2011a">[2]</a><a class="reference internal" href="2014/03/16/denorm.html#wu2012">[3]</a>.  The problems are essentially identical: given a predictive model with good rank-based performance (area under the ROC curve in the case of classifiers, Spearman’s <span class="math">\(\rho\)</span> or Kendall’s <span class="math">\(tau\)</span> in the case of regression problems with numeric response), find a transformation such that the output of that model accurately predicts the expected value of the quantity of interest.  Generally, a monotonic transformation is desired so that the rank-based performance is preserved, but one of the three methods described by the lab in question actually gives a transformation that is not strictly monotonic.  I don’t want to describe the methods in detail here.  The papers I cited are freely available online.  I mostly just want to share a few methods I implemented that are very similar (and were inspired by these scientists’ fine work).</p>
<div class="section" id="an-example">
<h2>An example</h2>
<p>To start with, let’s look at an extremely simplified example.  First I’m going to simulate a simple regression problem with a lognormal response distribution.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">calibrators</span> <span class="kn">import</span> <span class="n">SmoothIso</span><span class="p">,</span> <span class="n">SmoothMovingAverage</span><span class="p">,</span> <span class="n">spearman</span><span class="p">,</span> <span class="n">kendall</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># Generate some fake data with a lognormal distribution</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
<span class="n">eta</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">eta</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, I’m going to fit a linear regression model to the simulated data on the log scale.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Do a linear regression on the log of the data</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">z</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">z_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">)</span>
</pre></div>
</div>
<p>The goal now is to find a way to go from <span class="code docutils literal"><span class="pre">z_hat</span></span> back to some estimate of <span class="code docutils literal"><span class="pre">y</span></span>.  Of course we could have just done linear regression on the data scale directly, like this.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Try doing linear regression directly</span>
<span class="n">beta_hat_direct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_hat_direct</span><span class="p">)</span>

<span class="c"># Compare the two models</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">spearman</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z_hat</span><span class="p">)</span>
<span class="n">rho_direct</span> <span class="o">=</span> <span class="n">spearman</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">kendall</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z_hat</span><span class="p">)</span>
<span class="n">tau_direct</span> <span class="o">=</span> <span class="n">kendall</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="k">print</span> <span class="s">'rho is </span><span class="si">%f</span><span class="s"> for the log model and </span><span class="si">%f</span><span class="s"> for the direct model'</span> <span class="o">%</span> <span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">rho_direct</span><span class="p">)</span>
<span class="k">print</span> <span class="s">'tau is </span><span class="si">%f</span><span class="s"> for the log model and </span><span class="si">%f</span><span class="s"> for the direct model'</span> <span class="o">%</span> <span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">tau_direct</span><span class="p">)</span>
<span class="c"># rho is 0.802388 for the log model and 0.800836 for the direct model</span>
<span class="c"># tau is 0.606838 for the log model and 0.605232 for the direct model</span>
</pre></div>
</div>
<p>In this case, the difference in rank-based performance between the two models is not significant.  I just wanted to show how the comparison might be made.  When using more complex nonparametric methods, data scale has a significant effect on generalization ability of the fitted models.  With linear regression on this particular problem, overfitting is not really an issue.  Now I want to try out some different ways of reversing the log transformation on the predictions.  First, I’m going to try using the obvious and wrong inverse, <span class="math">\(\hat{y} = e^{\hat{z}}\)</span>.  It turns out that for this particular problem, where the data have a known lognormal distribution, the exact right answer is known.  That is, it is a provable fact that <span class="math">\(E\left(y\right) = e^{E\left(z\right) + \frac{\sigma^2}{2}}\)</span>, where <span class="math">\(\sigma\)</span> is the known scale parameter of the lognormal distribution.  I will use this result as a basis for comparison.  Next, I’m going to use the two methods I implemented, SmoothIso (based on the idea from <a class="reference internal" href="2014/03/16/denorm.html#jiang2011a">[2]</a>) and SmoothMovingAverage (based more loosely on the idea from <a class="reference internal" href="2014/03/16/denorm.html#jiang2011">[1]</a>).  Finally, I’ll plot the results together to see how they compare.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="c"># Range for plotting calibration curves</span>
<span class="n">z_range</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">z_hat</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="o">.</span><span class="mo">05</span><span class="p">)</span>

<span class="c"># Try reversing the log by inversion</span>
<span class="n">y_hat_inv</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Try reversing the log by the actual correct formula</span>
<span class="n">y_hat_correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_range</span> <span class="o">+</span> <span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c"># Try reversing the log using SmoothIso</span>
<span class="n">smooth_iso</span> <span class="o">=</span> <span class="n">SmoothIso</span><span class="p">(</span><span class="n">max_degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat_si</span> <span class="o">=</span> <span class="n">smooth_iso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Try reversing the log using SmoothMovingAverage</span>
<span class="n">moving_average</span> <span class="o">=</span> <span class="n">SmoothMovingAverage</span><span class="p">(</span><span class="n">max_degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat_sma</span> <span class="o">=</span> <span class="n">moving_average</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_range</span><span class="p">)</span>

<span class="c"># Plot the different reversal attempts</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'k.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_hat</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'direct regression'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_inv</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'$e^{\hat{z}}$'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_correct</span><span class="p">,</span> <span class="s">'r--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'$e^{\hat{z} + \sigma^{2}/2 }$'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_si</span><span class="p">,</span> <span class="s">'y'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'smooth iso'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_range</span><span class="p">,</span> <span class="n">y_hat_sma</span><span class="p">,</span> <span class="s">'g--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'smooth moving average'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'example.png'</span><span class="p">,</span> <span class="n">transparent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the resulting plot.</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/example.png"><img alt="`Plot of the different calibration methods`" src="_images/example.png" style="width: 600.0px; height: 450.0px;"/></a>
</div>
</div>
<div class="section" id="final-thoughts">
<h2>Final thoughts</h2>
<p>The plot illustrates nicely that simply inverting the original transformation is a very bad idea.  On this data set both the calibration methods stick near the correct (dotted red) curve.  What’s nice about this, of course, is that both the calibration methods are purely data driven, and work just as well in situations in which the exact distribution of the data is either not convenient or not known, or even in situations in which the original transformation is unknown.</p>
<p>So, how does all this work?  Well, the exact details are beyond the scope of this particular blog entry, but generally speaking SmoothIso works by performing isotonic regression followed by MARS, and SmoothMovingAverage works by training a MARS model on a moving average of the training data.  I encourage you to read the articles and check out my implementations.  I’m putting all the code used in this post, including the calibrators themselves, in a <a class="reference external" href="https://github.com/jcrudy/calibrators">github repository</a>.  I hope you’ll try it out.</p>
<p id="bibtex-bibliography-2014/03/16/denorm-0"><table class="docutils citation" frame="void" id="jiang2011" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine.. <em>Journal of the American Medical Informatics Association</em>, 19(2):263–274, 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jiang2011a" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id4">2</a>)</em> Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Smooth isotonic regression: a new method to calibrate predictive models.. <em>AMIA Summits Transl Sci Proc. 2011</em>, 2011:16–20, January 2011.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wu2012" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Yuan Wu, Xiaoqian Jiang, Jihoon Kim, and Lucila Ohno-machado. I-spline Smoothing for Calibrating Predictive Models. <em>AMIA Summits Transl Sci Proc. 2012</em>, pages 39–46, 2012.</td></tr>
</tbody>
</table>
</p>
</div>
</div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Jason Rudy</span>
        </div>
        <div class="categories">
            <span>
                Filed under:
                <a href="categories/python.html">python</a>, <a href="categories/statistics.html">statistics</a></span>
        </div>
        <div class="tags">
            <span>
                Tags:
                <a href="tags/transformation.html">transformation</a>, <a href="tags/calibration.html">calibration</a></span>
        </div>
        <div class="comments">
            <a href="http://jcrudy.github.io/blog/html/2014/03/16/denorm.html#disqus_thread" data-disqus-identifier="2014/03/16/denorm">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>February 07, 2014</span>
        </div>
        <div class="section" id="boosting-with-mars">
<h1><a href="2014/02/07/boosting_with_mars.html">Boosting with MARS</a></h1>
<p>Someone wrote me a few months ago asking for advice.  He was an R user who wanted to combine the adaboost algorithm with MARS and was starting to look outside the R community.  As a recovering (and occasionally relapsing) R user myself, I was only too happy to look into the matter.  As it turns out, it’s actually not that hard to do with a combination of scikit-learn and py-earth.  I now recount the adventure in detail.</p>
<p>Let it be known that this fine community member was interested in MARS not for regression, as is its usual role, but for classification.  This first challenge is easily accomplished using the Pipeline class, as follows.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">pyearth</span> <span class="kn">import</span> <span class="n">Earth</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">'earth'</span><span class="p">,</span><span class="n">Earth</span><span class="p">()),(</span><span class="s">'log'</span><span class="p">,</span><span class="n">LogisticRegression</span><span class="p">())])</span>
</pre></div>
</div>
<p>The above construction is equivalent to using the earth package from R with <cite>glm=list(family=binomial)</cite>.</p>
<p>Adaboost requires a classifier that can handle weighted samples.  The <cite>Earth</cite> class satisfies this requirement.  <cite>LogisticRegression</cite> does not, but we can replace it with <cite>SGDClassifier</cite> using <cite>loss=’log’</cite> to get an equivalent Pipeline that can handle sample weights.  Or, at least, it seems like it could.  As it turns out, though, the <cite>Pipeline</cite> class doesn’t actually know how to act as a base estimator in <cite>AdaBoostClassifier</cite>.  When you try to fit the resulting model, you get an error.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">pyearth</span> <span class="kn">import</span> <span class="n">Earth</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model.stochastic_gradient</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.weight_boosting</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>


<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">()</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">'earth'</span><span class="p">,</span><span class="n">Earth</span><span class="p">()),(</span><span class="s">'log'</span><span class="p">,</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">))])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">classifier</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>This code produces <cite>TypeError: base_estimator must be a subclass of ClassifierMixin</cite>.  You can get to the next error by making a special subclass of <cite>Pipeline</cite> like so.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">pyearth</span> <span class="kn">import</span> <span class="n">Earth</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model.stochastic_gradient</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.weight_boosting</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span>

<span class="k">class</span> <span class="nc">ClassifierPipeline</span><span class="p">(</span><span class="n">Pipeline</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">()</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">ClassifierPipeline</span><span class="p">([(</span><span class="s">'earth'</span><span class="p">,</span><span class="n">Earth</span><span class="p">()),(</span><span class="s">'log'</span><span class="p">,</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">))])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">classifier</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Now this code gives <cite>ValueError: need more than 1 value to unpack</cite>.  That’s somewhat more inscrutible.  To get around this one you actually have to rewrite part of the <cite>Pipeline</cite> class itself.  I posted a modified <cite>Pipeline</cite> as a <a class="reference external" href="https://gist.github.com/jcrudy/7493865#file-pipeline-py">gist</a>.  You can patch it in and use it like this.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">pipeline</span> <span class="kn">as</span> <span class="nn">alt_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">pyearth</span> <span class="kn">import</span> <span class="n">Earth</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model.stochastic_gradient</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.weight_boosting</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="k">class</span> <span class="nc">ClassifierPipeline</span><span class="p">(</span><span class="n">alt_pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">()</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">ClassifierPipeline</span><span class="p">([(</span><span class="s">'earth'</span><span class="p">,</span><span class="n">Earth</span><span class="p">()),(</span><span class="s">'log'</span><span class="p">,</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">))])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">classifier</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>So that’s how you use AdaBoost with MARS in Python.</p>
</div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Jason Rudy</span>
        </div>
        <div class="categories">
            <span>
                Filed under:
                <a href="categories/python.html">Python</a></span>
        </div>
        
        <div class="comments">
            <a href="http://jcrudy.github.io/blog/html/2014/02/07/boosting_with_mars.html#disqus_thread" data-disqus-identifier="2014/02/07/boosting_with_mars">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 08, 2013</span>
        </div>
        <div class="section" id="introduction-to-iscala">
<h1><a href="2013/12/08/introduction_to_iscala.html">Introduction to IScala</a></h1>
<p>Python is an amazingly productive glue language.  It’s become popular among data scientists over the past several years, partially because of great libraries like numpy, scipy, pandas, scikit-learn, statsmodels, etc., and partially because so much of data science consists simply of gluing things together.  There is a part of my life, though, that consists of actually implementing machine learning algorithms.  Python does not make that part easy.  In order to write efficient numerical code for Python, you need to move the majority of the computation out of Python and into statically typed, compiled code.  The best solution I’ve found is Cython, which compiles a Python-like language to C while taking care of the Python C api for you.  I wrote py-earth in Cython and am fairly happy with the result.  However, Cython is still a (really fantastic) hack.  Getting good performance out of Cython code requires knowledge of both C and the internals of Python, and most of the good stuff about Python - passing around functions, object-oriented design, readability - is lost when you start to really optimize.  If you want to combine concurrency with objects, you actually have to start passing pointers around.</p>
<p>I guess what I’m saying is I want to branch out a little.  I’d like to find a language where I can be productive both as a scientific user and as a scientific developer, something modern, new but not too new, mainstream but with just a bit of an edge.  Tall.  Anyway, it seems like there are two new languages out there vying for my affection.  I chose Scala over Julia for two reasons.  Firstly, Scala is a general purpose programming language.  While statisticians and academic scientists are very happy to use single purpose tools like R to do their analyses, data scientists need the ability to actually write software and integrate their analyses into larger projects.  Secondly, Scala is implemented on the JVM and compatible with Java.  That means I can use any Java library from within Scala and use Scala to write map-reduce queries for an Hadoop cluster without using the streaming interface.</p>
<p>So I’m exploring the Scala ecosystem and learning the language (which so far I think is fantastic).  My goal is to do some data analyses and implement one or two non-trivial algorithms in Scala.  But one step at a time.  Today I want to share <a class="reference external" href="https://github.com/mattpap/IScala">IScala</a>, which is basically just IPython with a Scala backend.  Although I actually rarely use IPython, I know a lot of people do and I suspect that Scala adoption will require the existence of a useful IPython replacement.</p>
<div class="section" id="installing">
<h2>Installing</h2>
<p>The IScala readme file lists several installation options.  The one that worked for me was as follows.  First, download the <a class="reference external" href="https://github.com/mattpap/IScala/releases">latest tarball</a> and unpack it somewhere.  I put it directly in my home directory.  Then create a scala profile for ipython:</p>
<div class="highlight-none"><div class="highlight"><pre>$ipython profile create scala
[ProfileCreate] Generating default config file: u'/Users/jason/.ipython/profile_scala/ipython_config.py'
[ProfileCreate] Generating default config file: u'/Users/jason/.ipython/profile_scala/ipython_qtconsole_config.py'
[ProfileCreate] Generating default config file: u'/Users/jason/.ipython/profile_scala/ipython_notebook_config.py'
[ProfileCreate] Generating default config file: u'/Users/jason/.ipython/profile_scala/ipython_nbconvert_config.py'
</pre></div>
</div>
<p>The output above tells you the location of the ipython_config.py file.  The next step is to edit ipython_config.py to tell IPython about the IScala kernel, as below:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Configuration file for ipython.</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>

<span class="c"># Use the IScala kernel</span>
<span class="hll"><span class="n">c</span><span class="o">.</span><span class="n">KernelManager</span><span class="o">.</span><span class="n">kernel_cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;java&quot;</span><span class="p">,</span> <span class="s">&quot;-jar&quot;</span><span class="p">,</span>
</span><span class="hll">                              <span class="s">&quot;$ISCALA_PATH/lib/IScala.jar&quot;</span><span class="p">,</span>
</span><span class="hll">                              <span class="s">&quot;--profile&quot;</span><span class="p">,</span>
</span><span class="hll">                              <span class="s">&quot;{connection_file}&quot;</span><span class="p">,</span>
</span><span class="hll">                              <span class="s">&quot;--parent&quot;</span><span class="p">]</span>
</span></pre></div>
</div>
<p>After you get this working, with $ISCALA_PATH replaced by the path to wherever you put the unpacked IScala download, you should be able to run IScala by saying:</p>
<div class="highlight-none"><div class="highlight"><pre>$ipython notebook --profile scala
</pre></div>
</div>
<p>or similarly for console or qtconsole.</p>
</div>
<div class="section" id="taking-it-for-a-spin">
<h2>Taking it for a spin</h2>
<p>I’m going to try generating a random matrix with a standard normal distribution.  The first thing I’ll need to do is import breeze.  My first attempt failed.</p>
<div class="iscala container">
<div class="code scala highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">breeze.linalg._</span>
<span class="kn">import</span> <span class="nn">breeze.stats.distributions._</span>
</pre></div>
</div>
<div class="highlight-python"><pre>&lt;console&gt;:7: error: not found: value breeze
       import breeze.linalg._
              ^
&lt;console&gt;:8: error: not found: value breeze
       import breeze.stats.distributions._
              ^</pre>
</div>
</div>
<p>I needed to tell sbt where to find breeze.  It turns out you can use IPython magic to talk to sbt like this.</p>
<div class="iscala container">
<div class="code scala highlight-python"><pre>%resolvers += &quot;ScalaNLP Maven2&quot; at &quot;http://repo.scalanlp.org/repo&quot;

%resolvers += &quot;Scala Tools Snapshots&quot; at &quot;http://scala-tools.org/repo-snapshots/&quot;

%resolvers += &quot;Typesafe Repository&quot; at &quot;http://repo.typesafe.com/typesafe/releases/&quot;

%resolvers += &quot;Sonatype Snapshots&quot; at &quot;https://oss.sonatype.org/content/repositories/snapshots&quot;

%libraryDependencies += &quot;org.scalanlp&quot; %% &quot;breeze&quot; % &quot;0.6-SNAPSHOT&quot;

%update</pre>
</div>
<div class="highlight-python"><pre>[info] Resolving org.scalanlp#breeze_2.10;0.6-SNAPSHOT ...
[info] Resolving org.scala-lang#scala-library;2.10.3 ...
[info] Resolving org.scalanlp#breeze-macros_2.10;0.1 ...
[info] Resolving org.scala-lang#scala-reflect;2.10.3 ...
[info] Resolving com.thoughtworks.paranamer#paranamer;2.2 ...
[info] Resolving com.github.fommil.netlib#all;1.1.2 ...
[info] Resolving net.sourceforge.f2j#arpack_combined_all;0.1 ...
[info] Resolving com.github.fommil.netlib#core;1.1.2 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#native_ref-java;1.1 ...
[info] Resolving com.github.fommil#jniloader;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#native_system-java;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 ...
[info] Resolving com.github.fommil.netlib#netlib-native_system-win-i686;1.1 ...
[info] Resolving org.scalanlp#lpsolve;5.5.2-SNAPSHOT ...
[info] Resolving net.sf.opencsv#opencsv;2.3 ...
[info] Resolving com.github.rwl#jtransforms;2.4.0 ...
[info] Resolving junit#junit;4.8.2 ...
[info] Resolving org.apache.commons#commons-math3;3.2 ...
[info] Resolving com.typesafe#scalalogging-slf4j_2.10;1.0.1 ...
[info] Resolving org.slf4j#slf4j-api;1.7.2 ...</pre>
</div>
<div class="code scala highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">breeze.linalg._</span>
<span class="kn">import</span> <span class="nn">breeze.stats.distributions._</span>
</pre></div>
</div>
</div>
<p>That worked!  I can now generate my random matrix like this:</p>
<div class="iscala container">
<div class="code scala highlight-python"><pre>val x = DenseMatrix.fill(10,10)(Gaussian(0,1).draw())</pre>
</div>
<div class="highlight-python"><pre>0.4695170376110142   0.5086639312405534    ... (10 total)
0.2604624080687952   -0.03678938632256435  ...
-1.0528337756159565  0.10082649287241126   ...
-0.550492849679171   -0.5761878622563654   ...
-0.9817603551889219  0.7958446618784706    ...
-1.0001995322763473  -1.2424889465651479   ...
-0.5879313146878662  1.206569217055404     ...
1.890300548243616    -0.30273380341887257  ...
0.24792587873136573  -0.04329745764599858  ...
1.5057194826425122   0.9516921598743895    ...</pre>
</div>
</div>
</div>
<div class="section" id="final-thoughts">
<h2>Final thoughts</h2>
<p>So IScala is up and running.  There doesn’t yet appear to be any support for displaying plots in the notebook, and it is a little annoying that I have to %update to add new dependencies (which causes the Scala process to restart and all existing objects to be lost from memory).  However, IScala is a new project and these are minor issues.  During the process I discovered something disconcerting about the Scala ecosystem, however.  There is not currently a Scala equivalent of numpy.  That is, there is no basic structure that everyone agrees is the standard backend array type.  Instead, there are several competing packages (of which <a class="reference external" href="http://www.scalanlp.org">breeze</a> is just one) that accomplish this extremely basic function.  I need to decide which one I want to develop on.</p>
</div>
</div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Jason Rudy</span>
        </div>
        
        
        <div class="comments">
            <a href="http://jcrudy.github.io/blog/html/2013/12/08/introduction_to_iscala.html#disqus_thread" data-disqus-identifier="2013/12/08/introduction_to_iscala">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 02, 2013</span>
        </div>
        <div class="section" id="blogstart">
<h1><a href="2013/12/02/blogstart.html">Blogstart</a></h1>
<p>Once in a great while a new blog is born, and today these rusty tubes are graced with a sparkling new presence.  It’s a little rough right now, but greatness can grow from meek beginnings.  Let us take a moment to remember some of the great weblogs of the past, whose unfathomable heights this humble site may hope one day to spy on the horizon.  <a class="reference external" href="http://jakevdp.github.io">Pythonic Perambulations</a>, <a class="reference external" href="http://twiecki.github.io">While My MCMC Gently Samples</a>, and all the rest, may I follow in your sizable footsteps.  This blog will be much in the spirit of these gentle giants.  It will concern my work and particularly my Python code.  It will contain short pieces of opinion, medium length demonstrations of technology, and longer analyses of data.  It will not be limited to Python or even to programming and statistics, but will tend to concern those topics more than any other.  At least that is my intention.</p>
<div class="section" id="blog-rules">
<h2>Blog rules <a class="footnote-reference" href="#f1" id="id1">[1]</a></h2>
<p>Every blog needs rules.  Here are some.</p>
<ol class="arabic simple">
<li>Trust no one.</li>
<li>Posts occur at least once per month.</li>
<li>Nothing is final.  If you want to see revision history, the <a class="reference external" href="https://github.com/jcrudy/jcrudy.github.io">blog is hosted on github</a>.</li>
</ol>
</div>
<div class="section" id="blog-implementation">
<h2>Blog implementation</h2>
<p>This blog, at least at the moment, is constructed using the <a class="reference external" href="http://tinkerer.me">Tinkerer</a> static blogging framework.  Why Tinkerer you ask?  When choosing a static blogging framework I considered the following options: Octopress, Jekyll, Hyde, Pelican, and Tinkerer.  I ruled out Octopress and Jekyll easily enough.  Although they’re both popular, they’re not Python and it would therefore be hard for me to read and understand the source code should the unthinkable occur.  Of the three remaining, I chose Tinkerer because it is based on Sphinx.  Sphinx is a mature and well supported Python tool for generating documentation.  Its main selling points for me are the easy support for math, syntax-highlighted code, bibtex, footnotes, and basically every other imaginable feature of a document via an impressive collection of plugins.  The downside of Tinkerer is that its relatively small userbase has produced very few nice looking themes.  However, I’ve decided theming is a secondary consideration for now.  Perhaps I can help to remedy the situation in the near future.</p>
<table class="docutils footnote" frame="void" id="f1" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>As with any ordered set of rules, the zeroeth rule is that there are no rules.</td></tr>
</tbody>
</table>
</div>
</div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Jason Rudy</span>
        </div>
        
        
        <div class="comments">
            <a href="http://jcrudy.github.io/blog/html/2013/12/02/blogstart.html#disqus_thread" data-disqus-identifier="2013/12/02/blogstart">Leave a comment</a>
        </div></div><div class="archive_link">
        <a href="archive.html"> &mdash; Blog Archive &mdash; </a>
    </div></article><aside class="sidebar"><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="2014/03/16/denorm.html">Transformations and consequences</a>
        </li><li>
            <a href="2014/02/07/boosting_with_mars.html">Boosting with MARS</a>
        </li><li>
            <a href="2013/12/08/introduction_to_iscala.html">Introduction to IScala</a>
        </li><li>
            <a href="2013/12/02/blogstart.html">Blogstart</a>
        </li></ul>
</div>
</section><section><div class="widget" id="searchbox">
    <h1>Search</h1>
    <form action="search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="webfont">L</span></button>
    </form>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container"><footer class="wrapper">&copy; Copyright 2013 and 2014, Jason Rudy. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><script type="text/javascript">    var disqus_shortname = "jcrudy-blog";    disqus_count();</script><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>